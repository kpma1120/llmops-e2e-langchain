from dotenv import load_dotenv
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_google_vertexai import ChatVertexAI

from vectorstore import vectorstore

load_dotenv()

# --- Retriever ---
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

# --- Chat Model ---
model = ChatVertexAI(model="gemini-2.5-pro", temperature=0)

# --- Prompt ---
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant answering questions about LangChain docs. "
               "Always cite sources from the retrieved context."),
    ("user", "Question: {question}\n\nContext:\n{context}")
])

refine_prompt = ChatPromptTemplate.from_messages([
    (
        "system", 
        "Refine the answer: remove repetition, fix formatting, and make it concise."
    ),
    ("user", "{answer}")
])

# --- LCEL RAG pipeline ---
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)

refine_chain = refine_prompt | model | StrOutputParser()

def run_llm(query: str) -> dict:
    """Run the RAG pipeline to generate an answer and cite sources.

    Args:
        query (str): The user query string.

    Returns:
        dict: A dictionary containing:
            - "answer" (str): The refined answer generated by the pipeline.
            - "sources" (list[str]): List of source identifiers for retrieved documents.
    """
    raw_answer = rag_chain.invoke(query)
    refined_answer = refine_chain.invoke({"answer": raw_answer})
    docs = retriever.invoke(query)
    sources = [doc.metadata.get("source", "Unknown") for doc in docs]
    return {"answer": refined_answer, "sources": sources}


if __name__ == "__main__":
    result = run_llm("What are deep agents?")
    print(result)
